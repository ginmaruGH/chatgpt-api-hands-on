{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fin-py/chatgpt-api-hands-on/blob/main/docs/models.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "LangChainではさまざまなモデルが扱えます。\n",
    "\n",
    "LLMs\n",
    "\n",
    ": 入力としてテキスト文字列を受け取り、出力としてテキスト文字列を返します。\n",
    "\n",
    "Chat Models\n",
    "\n",
    ": チャットメッセージのリストを入力として受け取り、チャットメッセージを返します。\n",
    "\n",
    "Text Embedding Models\n",
    "\n",
    ": テキストを入力とし、浮動小数点数のリストを返します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "`LLM` クラスはLLMのインタフェースです。OpenAIやCohereなどの異なるモデルでも標準的なインタフェースが提供されるよう設計されます。\n",
    "\n",
    "`OpenAI` クラスはOpenAIのLLMラッパーです。\n",
    "\n",
    "このモデルを使うにはOpenAIアカウントを作成し、 [API key](https://platform.openai.com/account/api-keys) を生成し、環境変数 `OPENAI_API_KEY` を設定します。\n",
    "\n",
    "Pythonパッケージをインストールします。\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI can speak japanese.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "openai_llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "openai_llm(\"Can you speak japanese?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HuggingFaceHub` クラスはHugging Face HubのLLMラッパーです。\n",
    "\n",
    "> https://huggingface.co/docs/hub/\n",
    "\n",
    "このモデルを使うにはHugging Faceのアカウントを作成し、 [Access Token](https://huggingface.co/settings/tokens) を生成し、環境変数 `HUGGINGFACEHUB_API_TOKEN` を設定します。\n",
    "\n",
    "Pythonパッケージをインストールします。\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, I understand Chinese. My boyfriend, I am here to speak some language in the future. This is for you, your boyfriend. I am going to let your friend take the next step. You two will'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "hf_llm = HuggingFaceHub(repo_id=\"gpt2\")\n",
    "hf_llm(\"Can you speak japanese?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "チャットモデルに1つ以上のメッセージを渡すことで、チャット結果を取得します。現在LangChainではHumanMessage、AIMessage、SystemMessageなどのメッセージがサポートされています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='私はペンを持っています。 (Watashi wa pen wo motteimasu.)', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to Japanese. I have a pen.\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAIのチャットモデルは、入力として複数のメッセージをサポートしています。\n",
    "\n",
    "> https://platform.openai.com/docs/guides/chat/chat-vs-completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='私はペンを持っています。(Watashi wa pen wo motte imasu.)', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to Japanese.\"),\n",
    "    HumanMessage(content=\"I have a pen.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate` を使って複数のメッセージに対する補完ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='私はペンを持っています。(Watashi wa pen wo motte imasu.)', generation_info=None, message=AIMessage(content='私はペンを持っています。(Watashi wa pen wo motte imasu.)', additional_kwargs={}, example=False))], [ChatGeneration(text='私はリンゴを持っています。(Watashi wa ringo wo motte imasu.)', generation_info=None, message=AIMessage(content='私はリンゴを持っています。(Watashi wa ringo wo motte imasu.)', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 58, 'completion_tokens': 41, 'total_tokens': 99}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Japanese.\"),\n",
    "        HumanMessage(content=\"I have a pen.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Japanese.\"),\n",
    "        HumanMessage(content=\"I have an Apple.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`llm_output` からトークンの使用量などが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 58,\n",
       "  'completion_tokens': 41,\n",
       "  'total_tokens': 99},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplates\n",
    "\n",
    "テンプレートが利用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='私はペンを持っています。(Watashi wa pen wo motte imasu.)', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Japanese\", text=\"I have a pen.\").to_messages())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LLMChain` を使って同様な処理ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私はペンを持っています。(Watashi wa pen wo motte imasu.)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"Japanese\", text=\"I have a pen.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "コールバック処理によりストリーミングをサポートしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♪ きのこ、たけのこ、おいしいおいしい\n",
      "　森の中で採って、食べよう\n",
      "　きのこ、たけのこ、栄養たっぷり\n",
      "　体にもいいから、食べよう\n",
      "\n",
      "　きのこ、たけのこ、色も形も違う\n",
      "　でも、どれも美味しいから、食べよう\n",
      "　きのこ、たけのこ、季節によって\n",
      "　味が変わるから、楽しみだね\n",
      "\n",
      "　きのこ、たけのこ、食べると元気になる\n",
      "　体に栄養を与えて、健康になろう\n",
      "　きのこ、たけのこ、自然の恵み\n",
      "　大切にして、美味しく食べよう"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"きのことたけのこの歌を作ってください\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding Models\n",
    "\n",
    "`Embedding` クラスはEmbeddingのインタフェースです。OpenAIやCohereなどのプロバイダに対応し、標準的なインタフェースが提供されるよう設計されています。\n",
    "\n",
    "Embeddingテキストの一部をベクトル表現にします。ベクトル空間にすることで似ているテキストを探したりします。\n",
    "\n",
    "`OpenAIEmbeddings` を使うには `tiktoken` をインストールします。\n",
    "\n",
    "```bash\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0031265460420399904,\n",
       " 0.01113363541662693,\n",
       " -0.004037691745907068,\n",
       " -0.011746617965400219,\n",
       " -0.0009935986017808318]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0031265460635144116,\n",
       " 0.011133635493097378,\n",
       " -0.004037691773639618,\n",
       " -0.011746618046080886,\n",
       " -0.000993598608605281]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result[0][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
